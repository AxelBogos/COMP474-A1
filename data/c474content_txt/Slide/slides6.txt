b'Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.1Lecture 6Recommender SystemsPersonalization, Collaborative Filtering & Content-based recommendationCOMP 474/6741, Winter 2021Ren\xc3\xa9 WitteDepartment of Computer Scienceand Software EngineeringConcordia UniversityRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.2Outline1 IntroductionModeling Users2 Collaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to other ItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluation3 Content-based RecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummary4 Notes and Further ReadingRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.3Slides CreditIncludes slides by Christopher D. Manning, Prabhakar Raghavan andHinrich Sch\xc3\xbctze [MRS08]\xe2\x80\xa2 Copyright \xc2\xa9 2008 Cambridge University PressRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.4Recommender Systems and Collaborative FilteringRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.5Collecting User InteractionsCopyright 2009 by Manning Publications Co., [Ala09]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.6Item MetadataCopyright 2009 by Manning Publications Co., [Ala09]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.7Netflix Recommendationshttps://www.youtube.com/watch?v=nq2QtatuF7Uhttps://www.youtube.com/watch?v=nq2QtatuF7URen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.81 Introduction2 Collaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to other ItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluation3 Content-based Recommendations4 Notes and Further ReadingRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.9Making RecommendationsGiven Information about a User. . .. . . we want to be able to have a system\xe2\x80\xa2 recommending items (books, movies, music, photos, videos, etc.)\xe2\x80\xa2 find users interested in a new item\xe2\x80\xa2 find similar items, based on interests of other usersRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.10Collaborative FilteringCopyright 2016 by Manning Publications Co., [TB16]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.11Data CollectionCopyright 2016 by Manning Publications Co., [TB16]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.12Fun with Flags VectorsVectorsA vector ~v is an element of a vector space.\xe2\x80\xa2 For example, ~v \xe2\x88\x88 Rn with~v =\xef\xa3\xae\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xb0x1x2...xn\xef\xa3\xb9\xef\xa3\xba\xef\xa3\xba\xef\xa3\xba\xef\xa3\xbb \xe2\x88\x88 RnVisualizationWe can visualize vectors, e.g., in 2D:~vRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.13So what?Vectors of words, users, products, . . .We can represent (users, documents, products) as vectors, e.g., using the count oftags or the weight of words. This is called a vector space model.\xe2\x80\xa2 Vector operations on entities, e.g., to compute their similarityCopyright 2008 by Cambridge University Press, [MRS08]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.14Movies as VectorsCopyright 2016 by Manning Publications Co., [TB16]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.15Length normalizationHow do we compute the length of a vector?\xe2\x80\xa2 A vector can be (length-) normalized by dividing each of its components by itslength \xe2\x80\x93 here we use the L2 norm: ||x ||2 =\xe2\x88\x9a\xe2\x88\x91i x2i\xe2\x80\xa2 This maps vectors onto the unit sphere . . .\xe2\x80\xa2 . . . since after normalization: ||x ||2 =\xe2\x88\x9a\xe2\x88\x91i x2i = 1.0\xe2\x80\xa2 As a result, longer and shorter vectors (more/fewer tags) have weights of thesame order of magnitude.\xe2\x86\x92 Worksheet #5: Tasks 1, 2Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.16How do we formalize vector space similarity?Computing the similarity\xe2\x80\xa2 First cut: (negative) distance between two points\xe2\x80\xa2 ( = distance between the end points of the two vectors)\xe2\x80\xa2 Euclidean distance?\xe2\x80\xa2 Euclidean distance is a bad idea . . .\xe2\x80\xa2 . . . because Euclidean distance is large for vectors of different lengths.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.17Why Euclidian distance is a bad idea0 101richpoorq: [rich poor]d1:Ranks of starving poets swelld2:Rich poor gap growsd3:Record baseball salaries in 2010The Euclidean distance of ~q and ~d2 is large although the distribution of terms in thequery q and the distribution of terms in the document d2 are very similar.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.18From angles to cosinesComparing vectors\xe2\x80\xa2 The following two notions are equivalent.\xe2\x80\xa2 Compare item vectors according to the angle between them, in decreasing order\xe2\x80\xa2 Rank item vectors according to cosine(item1, item2) in increasing order\xe2\x80\xa2 Cosine is a monotonically decreasing function of the angle for the interval[0\xe2\x97\xa6,180\xe2\x97\xa6]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.19CosineRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.20Cosine for normalized vectorsComputing similarity\xe2\x80\xa2 For normalized vectors, the cosine is equivalent to the dot product or scalarproduct.\xe2\x80\xa2 cos(~q, ~d) = ~q \xc2\xb7 ~d =\xe2\x88\x91i qi \xc2\xb7 di\xe2\x80\xa2 (if ~q and ~d are length-normalized).\xe2\x86\x92 Worksheet #5: Task 3Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.21Item RecommendationSimple Tag-based RecommendationCollaborative tagging gives rise to simple recommender approaches:\xe2\x80\xa2 show other items (products, photos, videos, music) that were tagged similar byother users\xe2\x80\xa2 exploited in many e-commerce/social networking web sitesRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.22Collaborative FilteringFinding related contentWhen multiple users tag the same resource, content can be discovered based onthe most frequent tags (example: Last.fm).Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.23RecommendationsRecommendations based on tagsWe can now exploit tags for a number of use cases:\xe2\x80\xa2 Recommend items related to other items\xe2\x80\xa2 Recommend items based on user\xe2\x80\x99s interest\xe2\x80\xa2 Find users interested in a new itemGeneral Approach\xe2\x80\xa2 Represent users/items as(normalized) term vectors\xe2\x80\xa2 Compute cosine similaritybetween vectors; i.e., the anglebetween them (for normalizedvectors, this is simply their dotproduct)Copyright 2008 by Cambridge University Press, [MRS08]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.24Items related to other itemsSimple point-to-point recommendation engine\xe2\x80\xa2 Create item vectors using raw count\xe2\x80\xa2 Normalize vectors\xe2\x80\xa2 Compute cosine similarityResult is a similarity matrixRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.25Items of interest to a userPersonalization\xe2\x80\xa2 Item-to-item is the same for all users\xe2\x80\xa2 How can we recommend items for a particular user?Solution: build user-specific similarity matrix\xe2\x80\xa2 computation of vectors, normalization as before\xe2\x80\xa2 this time, we calculate the cosine similarity between a user vector and articlevector\xe2\x86\x92 Worksheet #5: Tasks 4, 5Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.26Finding relevant users for an itemRecommending items to users\xe2\x80\xa2 New item comes in (blog post, photo, article, product, . . .)\xe2\x80\xa2 Which users would be interested in it?Similar to before, compute similarity matrix between metadata of new item andmetadata of users.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.27Cold-Start ProblemGeneral issue in recommender system deployment\xe2\x80\xa2 New user\xe2\x87\x92 no user profile for recommendations\xe2\x80\xa2 New item\xe2\x87\x92 no user interactions for this itemNo general solution. . .Some strategies:\xe2\x80\xa2 Ask user for preferences during sign-up\xe2\x80\xa2 Recommend top-n items (e.g., currently most popular movies/songs/products)Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.28Semantic Vocabularies for User ModelingSemantic User ProfilesIdea: Use vocabularies instead of keywords in the vector representation of a userprofileMotivation\xe2\x80\xa2 Semantic recommendations (remember the \xe2\x80\x9ctree\xe2\x80\x9d example)\xe2\x80\xa2 Open knowledge bases:\xe2\x80\xa2 interoperable between applications\xe2\x80\xa2 controlled by users, not corporationsRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.29VocabulariesGeneric user modeling vocabulariesFOAF\xe2\x80\xa2 The most popular generic user model offering descriptions for basic userinformation\xe2\x80\xa2 No comprehensive classes for describing preferences or interestsGUMO\xe2\x80\xa2 A generic user model that offers several classes for users\xe2\x80\x99 characteristics\xe2\x80\xa2 Basic user dimensions like Emotional States, Characteristics and PersonalityIntelLEO\xe2\x80\xa2 Several ontologies strongly focused on personalization\xe2\x80\xa2 Enables describing user and team modelling, preferences, tasks and interestsRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.30The $1m Netflix Prize CompetitionRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.31General machine learning processRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.32Performance EvaluationMeasuring performance\xe2\x80\xa2 Is our fancy model better than giving out random recommendations?\xe2\x80\xa2 We need metrics to evaluate and compare the performance of differentapproachesPrecision and RecallThe precision provides a measure of the quality of the generated recommendations:precision =#relevant system recommendations#all generated recommendationsThe recall indicates how many relevant recommendations were found by a system:recall =#correctly found recommendations#all relevant recommendationsGenerally, there is a trade-off between precision and recall.\xe2\x86\x92 Worksheet #5: Task 6Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.33Precision@kPrecision at cutoff k\xe2\x80\xa2 Return a ranked list of recommendations (based on cosine similarity)\xe2\x80\xa2 Evaluate only top-k recommendations (e.g., top-10)precision@k =1k\xc2\xb7k\xe2\x88\x91c=1rel(c),where rel(c) tells us if item at rank c was relevant (1) or not (0).Intuitively. . .The percentage of correct recommendations in the top-k .Wait, what happened to Recall?Well. . . in this application scenario, we don\xe2\x80\x99t really care (there are millions ofpotentially relevant items on Amazon or movies on Netflix)\xe2\x86\x92 Worksheet #5: Task 7Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.34Average PrecisionAverage Precision at NIf we recommend N items to a user, where there are at most m relevant items in1 . . .N,AP@N =1mN\xe2\x88\x91k=1precision@k \xc2\xb7rel(k)again, rel(c) is 1 if the recommendation at rank c is relevant, 0 otherwiseNoteAP \xe2\x80\x9crewards\xe2\x80\x9d (gives a higher score to) higher-ranked, correct recommendations\xe2\x86\x92 Worksheet #5: Task 8Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.35Mean Average Precision (MAP)MAP\xe2\x80\xa2 So far, everything was calculated for one user u \xe2\x88\x88 U\xe2\x80\xa2 But we want to know how well the system works across all users\xe2\x80\xa2 Hence, average the AP for all users:MAP@N =1|U||U|\xe2\x88\x91u=1AP@N(u)But wait, there\xe2\x80\x99s more. . .\xe2\x80\xa2 Accuracy, Sensitivity, F-measure, . . .\xe2\x80\xa2 Non-binary ranked results (i.e., not just correct or wrong, but a Likert-scale):Compute the discounted cumulative gain (DCG),DCGu = rel1 +|C|\xe2\x88\x91c=2relclog2 cRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.361 Introduction2 Collaborative Filtering3 Content-based RecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummary4 Notes and Further ReadingRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.37Content-based RecommendationsMotivation\xe2\x80\xa2 So far, we build our model using vectors of concepts (e.g., tags, moviecategories, etc.)\xe2\x80\xa2 What if we want to create recommendations based on the content\xe2\x80\xa2 Movie description/summary\xe2\x80\xa2 Blog post\xe2\x80\xa2 News article\xe2\x80\xa2 Research publication\xe2\x80\xa2 . . .ApproachSame idea, but now we have to build vectors out of whole documents\xe2\x80\xa2 Basic idea of information retrieval (IR)\xe2\x80\xa2 Used in Internet search enginesRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.38Binary incidence matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 1 1 0 0 0 1BRUTUS 1 1 0 1 0 0CAESAR 1 1 0 1 1 1CALPURNIA 0 1 0 0 0 0CLEOPATRA 1 0 0 0 0 0MERCY 1 0 1 1 1 1WORSER 1 0 1 1 1 0. . .Each document is represented as a binary vector \xe2\x88\x88 {0,1}|V |.[from Introduction to Information Retrieval]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.39Count matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 157 73 0 0 0 1BRUTUS 4 157 0 2 0 0CAESAR 232 227 0 2 1 0CALPURNIA 0 10 0 0 0 0CLEOPATRA 57 0 0 0 0 0MERCY 2 0 3 8 5 8WORSER 2 0 1 1 1 5. . .Each document is now represented as a count vector \xe2\x88\x88 N|V |.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.40Bag of words model\xe2\x80\xa2 We do not consider the order of words in a document.\xe2\x80\xa2 John is quicker than Mary and Mary is quicker than John are represented thesame way.\xe2\x80\xa2 This is called a bag of words model.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.41tf-idfTerm frequency tfThe term frequency tft,d of term t in document d is defined as the number of timesthat t occurs in d .Frequency in document vs. frequency in collection\xe2\x80\xa2 In addition, to term frequency (the frequency of the term in the document) . . .\xe2\x80\xa2 . . . we also want to use the frequency of the term in the collection for weightingand ranking.\xe2\x80\xa2 Rare terms are more informative than frequent terms.\xe2\x80\xa2 Consider a term in the query that is rare in the collection (e.g., ARACHNOCENTRIC).\xe2\x80\xa2 A document containing this term is very likely to be relevant.\xe2\x80\xa2 \xe2\x86\x92 We want high weights for rare terms like ARACHNOCENTRIC.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.42Desired weight for frequent termsWeighting scheme\xe2\x80\xa2 Frequent terms are less informative than rare terms.\xe2\x80\xa2 Consider a term in the query that is frequent in the collection (e.g., GOOD,INCREASE, LINE).\xe2\x80\xa2 A document containing this term is more likely to be relevant than a documentthat doesn\xe2\x80\x99t . . .\xe2\x80\xa2 . . . but words like GOOD, INCREASE and LINE are not sure indicators ofrelevance.\xe2\x80\xa2 \xe2\x86\x92 For frequent terms like GOOD, INCREASE, and LINE, we want positiveweights . . .\xe2\x80\xa2 . . . but lower weights than for rare terms.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.43Document FrequencyDocument Frequency (df)\xe2\x80\xa2 We want high weights for rare terms like ARACHNOCENTRIC.\xe2\x80\xa2 We want low (positive) weights for frequent words like GOOD, INCREASE, andLINE.\xe2\x80\xa2 We will use document frequency to factor this into computing the matchingscore.\xe2\x80\xa2 The document frequency is the number of documents in the collection that theterm occurs in.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.44idf weightinverse document frequency (idf)\xe2\x80\xa2 dft is the document frequency, the number of documents that t occurs in.\xe2\x80\xa2 dft is an inverse measure of the informativeness of term t .\xe2\x80\xa2 We define the idf weight of term t as follows:idft = log10Ndft(N is the number of documents in the collection.)\xe2\x80\xa2 idft is a measure of the informativeness of the term.\xe2\x80\xa2 [logN/dft ] instead of [N/dft ] to \xe2\x80\x9cdampen\xe2\x80\x9d the effect of idf\xe2\x80\xa2 Note that we use the log transformation for both term frequency and documentfrequency.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.45Examples for idfCompute idft using the formula: idft = log101,000,000dftterm dft idftcalpurnia 1 6animal 100 4sunday 1000 3fly 10,000 2under 100,000 1the 1,000,000 0Effect of idf on ranking\xe2\x80\xa2 idf affects the ranking of documents for queries with at least two terms.\xe2\x80\xa2 For example, in the query \xe2\x80\x9carachnocentric line\xe2\x80\x9d, idf weighting increases therelative weight of ARACHNOCENTRIC and decreases the relative weight of LINE.\xe2\x80\xa2 idf has little effect on ranking for one-term queries.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.46tf-idf weightingComputing tf-idf\xe2\x80\xa2 The tf-idf weight of a term is the product of its tf weight and its idf weight:wt,d = (1 + log tft,d ) \xc2\xb7 logNdft\xe2\x80\xa2 Best known weighting scheme in information retrieval\xe2\x80\xa2 Note: the \xe2\x80\x9c-\xe2\x80\x9d in tf-idf is a hyphen, not a minus sign!\xe2\x80\xa2 Alternative names: tf.idf, tf x idfRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.47Summary: tf-idf\xe2\x80\xa2 Assign a tf-idf weight for each term t in each document d :wt,d = (1 + log tft,d ) \xc2\xb7 log Ndft\xe2\x80\xa2 The tf-idf weight . . .\xe2\x80\xa2 . . . increases with the number of occurrences within a document. (term frequency)\xe2\x80\xa2 . . . increases with the rarity of the term in the collection. (inverse documentfrequency)\xe2\x86\x92 Worksheet #5: Task 9Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.48Binary incidence matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 1 1 0 0 0 1BRUTUS 1 1 0 1 0 0CAESAR 1 1 0 1 1 1CALPURNIA 0 1 0 0 0 0CLEOPATRA 1 0 0 0 0 0MERCY 1 0 1 1 1 1WORSER 1 0 1 1 1 0. . .Each document is represented as a binary vector \xe2\x88\x88 {0,1}|V |.[from Introduction to Information Retrieval]Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.49Count matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 157 73 0 0 0 1BRUTUS 4 157 0 2 0 0CAESAR 232 227 0 2 1 0CALPURNIA 0 10 0 0 0 0CLEOPATRA 57 0 0 0 0 0MERCY 2 0 3 8 5 8WORSER 2 0 1 1 1 5. . .Each document is now represented as a count vector \xe2\x88\x88 N|V |.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.50Binary\xe2\x86\x92 count\xe2\x86\x92 weight matrixAnthony Julius The Hamlet Othello Macbeth . . .and Caesar TempestCleopatraANTHONY 5.25 3.18 0.0 0.0 0.0 0.35BRUTUS 1.21 6.10 0.0 1.0 0.0 0.0CAESAR 8.59 2.54 0.0 1.51 0.25 0.0CALPURNIA 0.0 1.54 0.0 0.0 0.0 0.0CLEOPATRA 2.85 0.0 0.0 0.0 0.0 0.0MERCY 1.51 0.0 1.90 0.12 5.25 0.88WORSER 1.37 0.0 0.11 4.15 0.25 1.95. . .Each document is now represented as a real-valued vector of tf-idf weights \xe2\x88\x88 R|V |.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.51Vector Space ModelDocuments as vectors\xe2\x80\xa2 Each document is now represented as a real-valued vector of tf-idf weights\xe2\x88\x88 R|V |.\xe2\x80\xa2 So we have a |V |-dimensional real-valued vector space.\xe2\x80\xa2 Terms are axes of the space.\xe2\x80\xa2 Documents are points or vectors in this space.\xe2\x80\xa2 Very high-dimensional: tens of millions of dimensions when you apply this toweb search engines\xe2\x80\xa2 Each vector is very sparse \xe2\x80\x93 most entries are zero.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.52Queries as vectors\xe2\x80\xa2 Key idea 1: do the same for queries: represent them as vectors in thehigh-dimensional space\xe2\x80\xa2 Key idea 2: Rank documents according to their proximity to the query\xe2\x80\xa2 proximity = similarity\xe2\x80\xa2 proximity \xe2\x89\x88 negative distance\xe2\x80\xa2 Recall: We\xe2\x80\x99re doing this because we want to get away from theyou\xe2\x80\x99re-either-in-or-out, feast-or-famine Boolean model.\xe2\x80\xa2 Instead: rank relevant documents higher than nonrelevant documentsRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.53Cosine similarity between query and documentcos(~q, ~d) = SIM(~q, ~d) =~q \xc2\xb7 ~d|~q||~d |=\xe2\x88\x91|V |i=1 qidi\xe2\x88\x9a\xe2\x88\x91|V |i=1 q2i\xe2\x88\x9a\xe2\x88\x91|V |i=1 d2i\xe2\x80\xa2 qi is the tf-idf weight of term i in the query.\xe2\x80\xa2 di is the tf-idf weight of term i in the document.\xe2\x80\xa2 |~q| and |~d | are the lengths of ~q and ~d .\xe2\x80\xa2 This is the cosine similarity of ~q and ~d . . . . . . or, equivalently, the cosine of theangle between ~q and ~d .Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.54Cosine similarity illustrated0 101richpoor~v(q)~v(d1)~v(d2)~v(d3)\xce\xb8.Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.55Basic Recommender Engine using Vector Space ModelApproach\xe2\x80\xa2 Represent all documents (movie descriptions, blog posts, research articles,. . . ) as a weighted tf-idf vector\xe2\x80\xa2 Compute the cosine similarity between the target vector and each documentvector\xe2\x80\xa2 Rank documents with respect to the target\xe2\x80\xa2 Return the top k (e.g., k = 10) to the userRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.56SummaryVector Space Model\xe2\x80\xa2 A mathematical model to portray an n-dimensional space\xe2\x80\xa2 Entities are described by vectors with n coordinates in a real space Rn\xe2\x80\xa2 Given two vectors, we can compute a similarity coefficient between them\xe2\x80\xa2 Cosine of the angle between two vectors reflects their degree of similaritytf = 1 + log(tft,d ) (1)idf = logNdft(2)cos(~q , ~d ) =\xe2\x88\x91|v|i=1 qi \xc2\xb7 di\xe2\x88\x9a\xe2\x88\x91|v|i=1 q i2 \xc2\xb7\xe2\x88\x9a\xe2\x88\x91|v|i=1 d i2(3)Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.58Outline1 Introduction2 Collaborative Filtering3 Content-based Recommendations4 Notes and Further ReadingRen\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.59Reading MaterialRequired\xe2\x80\xa2 [Ala09, Chapters 2, 3] (Recommendations)\xe2\x80\xa2 [MRS08, Chapter 8] (Evaluation)Supplemental\xe2\x80\xa2 [MRS08, Chapter 6] (Vector Space Model, tf-idf)Ren\xc3\xa9 WitteIntroductionModeling UsersCollaborative FilteringIntroductionComputing with WordsItem RecommendationItems Related to otherItemsItems of Interest to a UserRelevant Users for an ItemSemantic User ProfilesEvaluationContent-basedRecommendationsMotivationTF*IDF weightingTerm Vector Space ModelSummaryNotes and FurtherReading6.60References[Ala09] Satnam Alag.Collective Intelligence in Action.Manning, 2009.https://concordiauniversity.on.worldcat.org/oclc/314121652.[MRS08] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\xc3\xbctze.Introduction to Information Retrieval.Cambridge University Press, 2008.http://informationretrieval.org.[TB16] Doug Turnbull and John Berryman.Relevant Search.Manning, 2016.https://concordiauniversity.on.worldcat.org/oclc/954339855.https://concordiauniversity.on.worldcat.org/oclc/314121652http://informationretrieval.orghttps://concordiauniversity.on.worldcat.org/oclc/954339855\tRecommender Systems\tIntroduction\tModeling Users\tCollaborative Filtering\tIntroduction\tComputing with Words\tItem Recommendation\tItems Related to other Items\tItems of Interest to a User\tRelevant Users for an Item\tSemantic User Profiles\tEvaluation\tContent-based Recommendations\tMotivation\tTF*IDF weighting\tTerm Vector Space Model\tSummary\tNotes and Further Reading'