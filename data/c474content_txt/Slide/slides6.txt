b'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.1\n\nLecture 6\nRecommender Systems\nPersonalization, Collaborative Filtering & Content-based recommendation\n\nCOMP 474/6741, Winter 2021\n\nRen\xc3\xa9 Witte\nDepartment of Computer Science\n\nand Software Engineering\nConcordia University\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.2\n\nOutline\n\n1 Introduction\nModeling Users\n\n2 Collaborative Filtering\nIntroduction\nComputing with Words\nItem Recommendation\nItems Related to other Items\nItems of Interest to a User\nRelevant Users for an Item\nSemantic User Profiles\nEvaluation\n\n3 Content-based Recommendations\nMotivation\nTF*IDF weighting\nTerm Vector Space Model\nSummary\n\n4 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.3\n\nSlides Credit\nIncludes slides by Christopher D. Manning, Prabhakar Raghavan and\nHinrich Sch\xc3\xbctze [MRS08]\n\n\xe2\x80\xa2 Copyright \xc2\xa9 2008 Cambridge University Press\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.4\n\nRecommender Systems and Collaborative Filtering\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.5\n\nCollecting User Interactions\n\nCopyright 2009 by Manning Publications Co., [Ala09]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.6\n\nItem Metadata\n\nCopyright 2009 by Manning Publications Co., [Ala09]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.7\n\nNetflix Recommendations\n\nhttps://www.youtube.com/watch?v=nq2QtatuF7U\n\nhttps://www.youtube.com/watch?v=nq2QtatuF7U\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.8\n\n1 Introduction\n\n2 Collaborative Filtering\nIntroduction\nComputing with Words\nItem Recommendation\nItems Related to other Items\nItems of Interest to a User\nRelevant Users for an Item\nSemantic User Profiles\nEvaluation\n\n3 Content-based Recommendations\n\n4 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.9\n\nMaking Recommendations\n\nGiven Information about a User. . .\n. . . we want to be able to have a system\n\n\xe2\x80\xa2 recommending items (books, movies, music, photos, videos, etc.)\n\xe2\x80\xa2 find users interested in a new item\n\xe2\x80\xa2 find similar items, based on interests of other users\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.10\n\nCollaborative Filtering\n\nCopyright 2016 by Manning Publications Co., [TB16]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.11\n\nData Collection\n\nCopyright 2016 by Manning Publications Co., [TB16]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.12\n\nFun with Flags Vectors\n\nVectors\nA vector ~v is an element of a vector space.\n\n\xe2\x80\xa2 For example, ~v \xe2\x88\x88 Rn with\n\n~v =\n\n\xef\xa3\xae\n\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xb0\n\nx1\nx2\n...\n\nxn\n\n\xef\xa3\xb9\n\xef\xa3\xba\xef\xa3\xba\xef\xa3\xba\xef\xa3\xbb \xe2\x88\x88 Rn\n\nVisualization\nWe can visualize vectors, e.g., in 2D:\n\n~v\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.13\n\nSo what?\n\nVectors of words, users, products, . . .\nWe can represent (users, documents, products) as vectors, e.g., using the count of\ntags or the weight of words. This is called a vector space model.\n\n\xe2\x80\xa2 Vector operations on entities, e.g., to compute their similarity\n\nCopyright 2008 by Cambridge University Press, [MRS08]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.14\n\nMovies as Vectors\n\nCopyright 2016 by Manning Publications Co., [TB16]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.15\n\nLength normalization\n\nHow do we compute the length of a vector?\n\n\xe2\x80\xa2 A vector can be (length-) normalized by dividing each of its components by its\n\nlength \xe2\x80\x93 here we use the L2 norm: ||x ||2 =\n\xe2\x88\x9a\xe2\x88\x91\n\ni x\n2\ni\n\n\xe2\x80\xa2 This maps vectors onto the unit sphere . . .\n\n\xe2\x80\xa2 . . . since after normalization: ||x ||2 =\n\xe2\x88\x9a\xe2\x88\x91\n\ni x\n2\ni = 1.0\n\n\xe2\x80\xa2 As a result, longer and shorter vectors (more/fewer tags) have weights of the\nsame order of magnitude.\n\n\xe2\x86\x92 Worksheet #5: Tasks 1, 2\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.16\n\nHow do we formalize vector space similarity?\n\nComputing the similarity\n\n\xe2\x80\xa2 First cut: (negative) distance between two points\n\xe2\x80\xa2 ( = distance between the end points of the two vectors)\n\xe2\x80\xa2 Euclidean distance?\n\xe2\x80\xa2 Euclidean distance is a bad idea . . .\n\xe2\x80\xa2 . . . because Euclidean distance is large for vectors of different lengths.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.17\n\nWhy Euclidian distance is a bad idea\n\n0 1\n0\n\n1\n\nrich\n\npoor\n\nq: [rich poor]\n\nd1:Ranks of starving poets swell\nd2:Rich poor gap grows\n\nd3:Record baseball salaries in 2010\n\nThe Euclidean distance of ~q and ~d2 is large although the distribution of terms in the\nquery q and the distribution of terms in the document d2 are very similar.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.18\n\nFrom angles to cosines\n\nComparing vectors\n\n\xe2\x80\xa2 The following two notions are equivalent.\n\xe2\x80\xa2 Compare item vectors according to the angle between them, in decreasing order\n\xe2\x80\xa2 Rank item vectors according to cosine(item1, item2) in increasing order\n\n\xe2\x80\xa2 Cosine is a monotonically decreasing function of the angle for the interval\n[0\xe2\x97\xa6,180\xe2\x97\xa6]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.19\n\nCosine\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.20\n\nCosine for normalized vectors\n\nComputing similarity\n\n\xe2\x80\xa2 For normalized vectors, the cosine is equivalent to the dot product or scalar\nproduct.\n\n\xe2\x80\xa2 cos(~q, ~d) = ~q \xc2\xb7 ~d =\n\xe2\x88\x91\n\ni qi \xc2\xb7 di\n\xe2\x80\xa2 (if ~q and ~d are length-normalized).\n\n\xe2\x86\x92 Worksheet #5: Task 3\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.21\n\nItem Recommendation\n\nSimple Tag-based Recommendation\nCollaborative tagging gives rise to simple recommender approaches:\n\n\xe2\x80\xa2 show other items (products, photos, videos, music) that were tagged similar by\nother users\n\n\xe2\x80\xa2 exploited in many e-commerce/social networking web sites\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.22\n\nCollaborative Filtering\n\nFinding related content\nWhen multiple users tag the same resource, content can be discovered based on\nthe most frequent tags (example: Last.fm).\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.23\n\nRecommendations\n\nRecommendations based on tags\nWe can now exploit tags for a number of use cases:\n\n\xe2\x80\xa2 Recommend items related to other items\n\xe2\x80\xa2 Recommend items based on user\xe2\x80\x99s interest\n\xe2\x80\xa2 Find users interested in a new item\n\nGeneral Approach\n\n\xe2\x80\xa2 Represent users/items as\n(normalized) term vectors\n\n\xe2\x80\xa2 Compute cosine similarity\nbetween vectors; i.e., the angle\nbetween them (for normalized\nvectors, this is simply their dot\nproduct)\n\nCopyright 2008 by Cambridge University Press, [MRS08]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.24\n\nItems related to other items\n\nSimple point-to-point recommendation engine\n\n\xe2\x80\xa2 Create item vectors using raw count\n\xe2\x80\xa2 Normalize vectors\n\xe2\x80\xa2 Compute cosine similarity\n\nResult is a similarity matrix\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.25\n\nItems of interest to a user\n\nPersonalization\n\n\xe2\x80\xa2 Item-to-item is the same for all users\n\xe2\x80\xa2 How can we recommend items for a particular user?\n\nSolution: build user-specific similarity matrix\n\xe2\x80\xa2 computation of vectors, normalization as before\n\xe2\x80\xa2 this time, we calculate the cosine similarity between a user vector and article\n\nvector\n\n\xe2\x86\x92 Worksheet #5: Tasks 4, 5\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.26\n\nFinding relevant users for an item\n\nRecommending items to users\n\n\xe2\x80\xa2 New item comes in (blog post, photo, article, product, . . .)\n\xe2\x80\xa2 Which users would be interested in it?\n\nSimilar to before, compute similarity matrix between metadata of new item and\nmetadata of users.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.27\n\nCold-Start Problem\n\nGeneral issue in recommender system deployment\n\n\xe2\x80\xa2 New user\xe2\x87\x92 no user profile for recommendations\n\xe2\x80\xa2 New item\xe2\x87\x92 no user interactions for this item\n\nNo general solution. . .\nSome strategies:\n\n\xe2\x80\xa2 Ask user for preferences during sign-up\n\xe2\x80\xa2 Recommend top-n items (e.g., currently most popular movies/songs/products)\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.28\n\nSemantic Vocabularies for User Modeling\n\nSemantic User Profiles\nIdea: Use vocabularies instead of keywords in the vector representation of a user\nprofile\n\nMotivation\n\n\xe2\x80\xa2 Semantic recommendations (remember the \xe2\x80\x9ctree\xe2\x80\x9d example)\n\xe2\x80\xa2 Open knowledge bases:\n\n\xe2\x80\xa2 interoperable between applications\n\xe2\x80\xa2 controlled by users, not corporations\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.29\n\nVocabularies\n\nGeneric user modeling vocabularies\nFOAF\n\n\xe2\x80\xa2 The most popular generic user model offering descriptions for basic user\ninformation\n\n\xe2\x80\xa2 No comprehensive classes for describing preferences or interests\nGUMO\n\n\xe2\x80\xa2 A generic user model that offers several classes for users\xe2\x80\x99 characteristics\n\xe2\x80\xa2 Basic user dimensions like Emotional States, Characteristics and Personality\n\nIntelLEO\n\xe2\x80\xa2 Several ontologies strongly focused on personalization\n\xe2\x80\xa2 Enables describing user and team modelling, preferences, tasks and interests\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.30\n\nThe $1m Netflix Prize Competition\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.31\n\nGeneral machine learning process\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.32\n\nPerformance Evaluation\n\nMeasuring performance\n\n\xe2\x80\xa2 Is our fancy model better than giving out random recommendations?\n\xe2\x80\xa2 We need metrics to evaluate and compare the performance of different\n\napproaches\n\nPrecision and Recall\nThe precision provides a measure of the quality of the generated recommendations:\n\nprecision =\n#relevant system recommendations\n\n#all generated recommendations\n\nThe recall indicates how many relevant recommendations were found by a system:\n\nrecall =\n#correctly found recommendations\n\n#all relevant recommendations\n\nGenerally, there is a trade-off between precision and recall.\n\n\xe2\x86\x92 Worksheet #5: Task 6\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.33\n\nPrecision@k\n\nPrecision at cutoff k\n\n\xe2\x80\xa2 Return a ranked list of recommendations (based on cosine similarity)\n\xe2\x80\xa2 Evaluate only top-k recommendations (e.g., top-10)\n\nprecision@k =\n1\nk\n\xc2\xb7\n\nk\xe2\x88\x91\nc=1\n\nrel(c),\n\nwhere rel(c) tells us if item at rank c was relevant (1) or not (0).\n\nIntuitively. . .\nThe percentage of correct recommendations in the top-k .\n\nWait, what happened to Recall?\nWell. . . in this application scenario, we don\xe2\x80\x99t really care (there are millions of\npotentially relevant items on Amazon or movies on Netflix)\n\n\xe2\x86\x92 Worksheet #5: Task 7\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.34\n\nAverage Precision\n\nAverage Precision at N\nIf we recommend N items to a user, where there are at most m relevant items in\n1 . . .N,\n\nAP@N =\n1\nm\n\nN\xe2\x88\x91\nk=1\n\nprecision@k \xc2\xb7rel(k)\n\nagain, rel(c) is 1 if the recommendation at rank c is relevant, 0 otherwise\n\nNote\nAP \xe2\x80\x9crewards\xe2\x80\x9d (gives a higher score to) higher-ranked, correct recommendations\n\n\xe2\x86\x92 Worksheet #5: Task 8\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.35\n\nMean Average Precision (MAP)\n\nMAP\n\n\xe2\x80\xa2 So far, everything was calculated for one user u \xe2\x88\x88 U\n\xe2\x80\xa2 But we want to know how well the system works across all users\n\xe2\x80\xa2 Hence, average the AP for all users:\n\nMAP@N =\n1\n|U|\n\n|U|\xe2\x88\x91\nu=1\n\nAP@N(u)\n\nBut wait, there\xe2\x80\x99s more. . .\n\n\xe2\x80\xa2 Accuracy, Sensitivity, F-measure, . . .\n\xe2\x80\xa2 Non-binary ranked results (i.e., not just correct or wrong, but a Likert-scale):\n\nCompute the discounted cumulative gain (DCG),\n\nDCGu = rel1 +\n|C|\xe2\x88\x91\nc=2\n\nrelc\nlog2 c\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.36\n\n1 Introduction\n\n2 Collaborative Filtering\n\n3 Content-based Recommendations\nMotivation\nTF*IDF weighting\nTerm Vector Space Model\nSummary\n\n4 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.37\n\nContent-based Recommendations\n\nMotivation\n\n\xe2\x80\xa2 So far, we build our model using vectors of concepts (e.g., tags, movie\ncategories, etc.)\n\n\xe2\x80\xa2 What if we want to create recommendations based on the content\n\xe2\x80\xa2 Movie description/summary\n\xe2\x80\xa2 Blog post\n\xe2\x80\xa2 News article\n\xe2\x80\xa2 Research publication\n\xe2\x80\xa2 . . .\n\nApproach\nSame idea, but now we have to build vectors out of whole documents\n\n\xe2\x80\xa2 Basic idea of information retrieval (IR)\n\xe2\x80\xa2 Used in Internet search engines\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.38\n\nBinary incidence matrix\n\nAnthony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\n\nCleopatra\nANTHONY 1 1 0 0 0 1\nBRUTUS 1 1 0 1 0 0\nCAESAR 1 1 0 1 1 1\nCALPURNIA 0 1 0 0 0 0\nCLEOPATRA 1 0 0 0 0 0\nMERCY 1 0 1 1 1 1\nWORSER 1 0 1 1 1 0\n. . .\n\nEach document is represented as a binary vector \xe2\x88\x88 {0,1}|V |.\n[from Introduction to Information Retrieval]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.39\n\nCount matrix\n\nAnthony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\n\nCleopatra\nANTHONY 157 73 0 0 0 1\nBRUTUS 4 157 0 2 0 0\nCAESAR 232 227 0 2 1 0\nCALPURNIA 0 10 0 0 0 0\nCLEOPATRA 57 0 0 0 0 0\nMERCY 2 0 3 8 5 8\nWORSER 2 0 1 1 1 5\n. . .\n\nEach document is now represented as a count vector \xe2\x88\x88 N|V |.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.40\n\nBag of words model\n\n\xe2\x80\xa2 We do not consider the order of words in a document.\n\xe2\x80\xa2 John is quicker than Mary and Mary is quicker than John are represented the\n\nsame way.\n\xe2\x80\xa2 This is called a bag of words model.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.41\n\ntf-idf\n\nTerm frequency tf\nThe term frequency tft,d of term t in document d is defined as the number of times\nthat t occurs in d .\n\nFrequency in document vs. frequency in collection\n\n\xe2\x80\xa2 In addition, to term frequency (the frequency of the term in the document) . . .\n\xe2\x80\xa2 . . . we also want to use the frequency of the term in the collection for weighting\n\nand ranking.\n\xe2\x80\xa2 Rare terms are more informative than frequent terms.\n\n\xe2\x80\xa2 Consider a term in the query that is rare in the collection (e.g., ARACHNOCENTRIC).\n\xe2\x80\xa2 A document containing this term is very likely to be relevant.\n\xe2\x80\xa2 \xe2\x86\x92 We want high weights for rare terms like ARACHNOCENTRIC.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.42\n\nDesired weight for frequent terms\n\nWeighting scheme\n\n\xe2\x80\xa2 Frequent terms are less informative than rare terms.\n\xe2\x80\xa2 Consider a term in the query that is frequent in the collection (e.g., GOOD,\n\nINCREASE, LINE).\n\xe2\x80\xa2 A document containing this term is more likely to be relevant than a document\n\nthat doesn\xe2\x80\x99t . . .\n\xe2\x80\xa2 . . . but words like GOOD, INCREASE and LINE are not sure indicators of\n\nrelevance.\n\xe2\x80\xa2 \xe2\x86\x92 For frequent terms like GOOD, INCREASE, and LINE, we want positive\n\nweights . . .\n\xe2\x80\xa2 . . . but lower weights than for rare terms.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.43\n\nDocument Frequency\n\nDocument Frequency (df)\n\n\xe2\x80\xa2 We want high weights for rare terms like ARACHNOCENTRIC.\n\xe2\x80\xa2 We want low (positive) weights for frequent words like GOOD, INCREASE, and\n\nLINE.\n\xe2\x80\xa2 We will use document frequency to factor this into computing the matching\n\nscore.\n\xe2\x80\xa2 The document frequency is the number of documents in the collection that the\n\nterm occurs in.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.44\n\nidf weight\n\ninverse document frequency (idf)\n\n\xe2\x80\xa2 dft is the document frequency, the number of documents that t occurs in.\n\xe2\x80\xa2 dft is an inverse measure of the informativeness of term t .\n\xe2\x80\xa2 We define the idf weight of term t as follows:\n\nidft = log10\nN\ndft\n\n(N is the number of documents in the collection.)\n\xe2\x80\xa2 idft is a measure of the informativeness of the term.\n\xe2\x80\xa2 [logN/dft ] instead of [N/dft ] to \xe2\x80\x9cdampen\xe2\x80\x9d the effect of idf\n\xe2\x80\xa2 Note that we use the log transformation for both term frequency and document\n\nfrequency.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.45\n\nExamples for idf\n\nCompute idft using the formula: idft = log10\n1,000,000\n\ndft\n\nterm dft idft\ncalpurnia 1 6\nanimal 100 4\nsunday 1000 3\nfly 10,000 2\nunder 100,000 1\nthe 1,000,000 0\n\nEffect of idf on ranking\n\n\xe2\x80\xa2 idf affects the ranking of documents for queries with at least two terms.\n\xe2\x80\xa2 For example, in the query \xe2\x80\x9carachnocentric line\xe2\x80\x9d, idf weighting increases the\n\nrelative weight of ARACHNOCENTRIC and decreases the relative weight of LINE.\n\xe2\x80\xa2 idf has little effect on ranking for one-term queries.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.46\n\ntf-idf weighting\n\nComputing tf-idf\n\n\xe2\x80\xa2 The tf-idf weight of a term is the product of its tf weight and its idf weight:\n\nwt,d = (1 + log tft,d ) \xc2\xb7 log\nN\ndft\n\n\xe2\x80\xa2 Best known weighting scheme in information retrieval\n\xe2\x80\xa2 Note: the \xe2\x80\x9c-\xe2\x80\x9d in tf-idf is a hyphen, not a minus sign!\n\xe2\x80\xa2 Alternative names: tf.idf, tf x idf\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.47\n\nSummary: tf-idf\n\n\xe2\x80\xa2 Assign a tf-idf weight for each term t in each document d :\nwt,d = (1 + log tft,d ) \xc2\xb7 log Ndft\n\n\xe2\x80\xa2 The tf-idf weight . . .\n\xe2\x80\xa2 . . . increases with the number of occurrences within a document. (term frequency)\n\xe2\x80\xa2 . . . increases with the rarity of the term in the collection. (inverse document\n\nfrequency)\n\n\xe2\x86\x92 Worksheet #5: Task 9\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.48\n\nBinary incidence matrix\n\nAnthony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\n\nCleopatra\nANTHONY 1 1 0 0 0 1\nBRUTUS 1 1 0 1 0 0\nCAESAR 1 1 0 1 1 1\nCALPURNIA 0 1 0 0 0 0\nCLEOPATRA 1 0 0 0 0 0\nMERCY 1 0 1 1 1 1\nWORSER 1 0 1 1 1 0\n. . .\n\nEach document is represented as a binary vector \xe2\x88\x88 {0,1}|V |.\n[from Introduction to Information Retrieval]\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.49\n\nCount matrix\n\nAnthony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\n\nCleopatra\nANTHONY 157 73 0 0 0 1\nBRUTUS 4 157 0 2 0 0\nCAESAR 232 227 0 2 1 0\nCALPURNIA 0 10 0 0 0 0\nCLEOPATRA 57 0 0 0 0 0\nMERCY 2 0 3 8 5 8\nWORSER 2 0 1 1 1 5\n. . .\n\nEach document is now represented as a count vector \xe2\x88\x88 N|V |.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.50\n\nBinary\xe2\x86\x92 count\xe2\x86\x92 weight matrix\n\nAnthony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\n\nCleopatra\nANTHONY 5.25 3.18 0.0 0.0 0.0 0.35\nBRUTUS 1.21 6.10 0.0 1.0 0.0 0.0\nCAESAR 8.59 2.54 0.0 1.51 0.25 0.0\nCALPURNIA 0.0 1.54 0.0 0.0 0.0 0.0\nCLEOPATRA 2.85 0.0 0.0 0.0 0.0 0.0\nMERCY 1.51 0.0 1.90 0.12 5.25 0.88\nWORSER 1.37 0.0 0.11 4.15 0.25 1.95\n. . .\n\nEach document is now represented as a real-valued vector of tf-idf weights \xe2\x88\x88 R|V |.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.51\n\nVector Space Model\n\nDocuments as vectors\n\n\xe2\x80\xa2 Each document is now represented as a real-valued vector of tf-idf weights\n\xe2\x88\x88 R|V |.\n\n\xe2\x80\xa2 So we have a |V |-dimensional real-valued vector space.\n\xe2\x80\xa2 Terms are axes of the space.\n\xe2\x80\xa2 Documents are points or vectors in this space.\n\xe2\x80\xa2 Very high-dimensional: tens of millions of dimensions when you apply this to\n\nweb search engines\n\xe2\x80\xa2 Each vector is very sparse \xe2\x80\x93 most entries are zero.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.52\n\nQueries as vectors\n\n\xe2\x80\xa2 Key idea 1: do the same for queries: represent them as vectors in the\nhigh-dimensional space\n\n\xe2\x80\xa2 Key idea 2: Rank documents according to their proximity to the query\n\xe2\x80\xa2 proximity = similarity\n\xe2\x80\xa2 proximity \xe2\x89\x88 negative distance\n\xe2\x80\xa2 Recall: We\xe2\x80\x99re doing this because we want to get away from the\n\nyou\xe2\x80\x99re-either-in-or-out, feast-or-famine Boolean model.\n\xe2\x80\xa2 Instead: rank relevant documents higher than nonrelevant documents\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.53\n\nCosine similarity between query and document\n\ncos(~q, ~d) = SIM(~q, ~d) =\n~q \xc2\xb7 ~d\n|~q||~d |\n\n=\n\n\xe2\x88\x91|V |\ni=1 qidi\xe2\x88\x9a\xe2\x88\x91|V |\n\ni=1 q\n2\ni\n\n\xe2\x88\x9a\xe2\x88\x91|V |\ni=1 d\n\n2\ni\n\n\xe2\x80\xa2 qi is the tf-idf weight of term i in the query.\n\xe2\x80\xa2 di is the tf-idf weight of term i in the document.\n\n\xe2\x80\xa2 |~q| and |~d | are the lengths of ~q and ~d .\n\xe2\x80\xa2 This is the cosine similarity of ~q and ~d . . . . . . or, equivalently, the cosine of the\n\nangle between ~q and ~d .\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.54\n\nCosine similarity illustrated\n\n0 1\n0\n\n1\n\nrich\n\npoor\n\n~v(q)\n\n~v(d1)\n\n~v(d2)\n\n~v(d3)\n\n\xce\xb8\n\n.\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.55\n\nBasic Recommender Engine using Vector Space Model\n\nApproach\n\n\xe2\x80\xa2 Represent all documents (movie descriptions, blog posts, research articles,\n. . . ) as a weighted tf-idf vector\n\n\xe2\x80\xa2 Compute the cosine similarity between the target vector and each document\nvector\n\n\xe2\x80\xa2 Rank documents with respect to the target\n\xe2\x80\xa2 Return the top k (e.g., k = 10) to the user\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.56\n\nSummary\n\nVector Space Model\n\n\xe2\x80\xa2 A mathematical model to portray an n-dimensional space\n\n\xe2\x80\xa2 Entities are described by vectors with n coordinates in a real space Rn\n\n\xe2\x80\xa2 Given two vectors, we can compute a similarity coefficient between them\n\n\xe2\x80\xa2 Cosine of the angle between two vectors reflects their degree of similarity\n\ntf = 1 + log(tft,d ) (1)\n\nidf = log\nN\ndft\n\n(2)\n\ncos(~q , ~d ) =\n\xe2\x88\x91|v|\n\ni=1 qi \xc2\xb7 di\xe2\x88\x9a\xe2\x88\x91|v|\ni=1 q i\n\n2 \xc2\xb7\n\xe2\x88\x9a\xe2\x88\x91|v|\n\ni=1 d i\n2\n\n(3)\n\n\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.58\n\nOutline\n\n1 Introduction\n\n2 Collaborative Filtering\n\n3 Content-based Recommendations\n\n4 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.59\n\nReading Material\n\nRequired\n\n\xe2\x80\xa2 [Ala09, Chapters 2, 3] (Recommendations)\n\xe2\x80\xa2 [MRS08, Chapter 8] (Evaluation)\n\nSupplemental\n\n\xe2\x80\xa2 [MRS08, Chapter 6] (Vector Space Model, tf-idf)\n\n\n\nRen\xc3\xa9 Witte\n\nIntroduction\nModeling Users\n\nCollaborative Filtering\nIntroduction\n\nComputing with Words\n\nItem Recommendation\n\nItems Related to other\nItems\n\nItems of Interest to a User\n\nRelevant Users for an Item\n\nSemantic User Profiles\n\nEvaluation\n\nContent-based\nRecommendations\nMotivation\n\nTF*IDF weighting\n\nTerm Vector Space Model\n\nSummary\n\nNotes and Further\nReading\n\n6.60\n\nReferences\n\n[Ala09] Satnam Alag.\nCollective Intelligence in Action.\nManning, 2009.\nhttps://concordiauniversity.on.worldcat.org/oclc/314121652.\n\n[MRS08] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\xc3\xbctze.\nIntroduction to Information Retrieval.\nCambridge University Press, 2008.\nhttp://informationretrieval.org.\n\n[TB16] Doug Turnbull and John Berryman.\nRelevant Search.\nManning, 2016.\nhttps://concordiauniversity.on.worldcat.org/oclc/954339855.\n\nhttps://concordiauniversity.on.worldcat.org/oclc/314121652\nhttp://informationretrieval.org\nhttps://concordiauniversity.on.worldcat.org/oclc/954339855\n\n\tRecommender Systems\n\tIntroduction\n\tModeling Users\n\n\tCollaborative Filtering\n\tIntroduction\n\tComputing with Words\n\tItem Recommendation\n\tItems Related to other Items\n\tItems of Interest to a User\n\tRelevant Users for an Item\n\tSemantic User Profiles\n\tEvaluation\n\n\tContent-based Recommendations\n\tMotivation\n\tTF*IDF weighting\n\tTerm Vector Space Model\n\tSummary\n\n\tNotes and Further Reading\n\n\n'