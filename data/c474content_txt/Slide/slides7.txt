b'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.1\n\nLecture 7\nMachine Learning for Intelligent Systems\nIntroduction, Clustering, Classification, Regression, Evaluation\n\nCOMP 474/6741, Winter 2021\n\nRen\xc3\xa9 Witte\nDepartment of Computer Science\n\nand Software Engineering\nConcordia University\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.2\n\nOutline\n1 Machine Learning Primer\n\nHistory\nML Types\nProcess\n\n2 Clustering Documents\nMotivation\nk-Means Clustering\nApplication Example\n\n3 Classifications & Predictions\nIntroduction\nClassification with kNN\nRegression with kNN\n\n4 Machine Learning Evaluation\nEvaluation Methodology\nEvaluation Metrics\nError Analysis\nOverfitting\nUnderfitting\nCross-Validation\n\n5 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.3\n\nAI, ML, DL\n\nhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b\n\nhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.4\n\nHistory\n\nLearn from experience\nIn 1959, Arthur Samuel first proposed the concept\nMachine Learning:\n\n\xe2\x80\x9cA computer program is said to learn from\nexperience E with respect to some class of tasks\nT and performance measure P if its performance\nat tasks in T, as measured by P, improves with\nexperience E.\xe2\x80\x9d\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.5\n\nAutomated Reasoning\n\nInference\nProcess of deriving new facts from a set of premises\n\nTypes of logical inference\n\n1 Deduction\n2 Abduction\n3 Induction\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.6\n\nDeduction\n\naka Natural Deduction\n\n\xe2\x80\xa2 Conclusion follows necessary from the premises.\n\xe2\x80\xa2 From A\xe2\x87\x92 B and A, we conclude that B\n\xe2\x80\xa2 We conclude from the general case to a specific example of the general case\n\xe2\x80\xa2 Example:\n\n1 All men are mortal.\n2 Socrates is a man.\n3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 Socrates is mortal.\n\n\xe2\x80\xa2 Our subclass inference in RDFS also falls into this category.\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.7\n\nAbduction\n\nAbductive Reasoning\n\n\xe2\x80\xa2 Conclusion is one hypothetical (most probable) explanation for the premises\n\xe2\x80\xa2 From A\xe2\x87\x92 B and B, we conclude A\n\xe2\x80\xa2 Example:\n\n1 Drunk people do not walk straight.\n2 John does not walk straight.\n3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 John is drunk.\n\n\xe2\x80\xa2 Not sound. . . but may be most likely explanation for B\n\xe2\x80\xa2 Used in medicine. . .\n\n1 in reality: disease\xe2\x87\x92 symptoms\n2 patient complains about some symptoms. . . doctor concludes a disease\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.8\n\nInduction\n\nInductive Reasoning\n\n\xe2\x80\xa2 Conclusion about all members of a class from the examination of only a few\nmember of the class.\n\n\xe2\x80\xa2 From A\xe2\x88\xa7C\xe2\x87\x92 B and A\xe2\x88\xa7D\xe2\x87\x92 B, we conclude A\xe2\x87\x92B\n\xe2\x80\xa2 We construct a general explanation based on specific cases\n\xe2\x80\xa2 Example:\n\n1 All CS students in COMP 474 are smart.\n2 All CS students on vacation are smart.\n3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 All CS students are smart.\n\n\xe2\x80\xa2 Not sound\n\xe2\x80\xa2 But, can be seen as hypothesis construction or generalisation\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.9\n\nInductive Learning\n\nLearning from examples\n\n\xe2\x80\xa2 Most work in ML\n\xe2\x80\xa2 Examples are given (positive and/or negative) to train a system in a\n\nclassification (or regression) task\n\xe2\x80\xa2 Extrapolate from the training set to make accurate predictions about future\n\nexamples\n\xe2\x80\xa2 Given a new instance X you have never seen, you must find an estimate of the\n\nfunction f(X) where f(X) is the desired output\n\nFrom datascience.com, https://towardsdatascience.com/cat- dog- or- elon- musk- 145658489730\n\nhttps://towardsdatascience.com/cat-dog-or-elon-musk-145658489730\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.10\n\nExample\n\n\xe2\x80\xa2 Given pairs (X , f (X )) (the training set \xe2\x80\x93 the data points)\n\xe2\x80\xa2 Find a function f that fits the training set well\n\xe2\x80\xa2 So that given a new X , you can predict its f (X ) value\n\nNote: choosing one function over another beyond just looking at the training set is\ncalled inductive bias (eg. prefer \xe2\x80\x9csmoother\xe2\x80\x9d functions)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.11\n\nInductive Learning Framework\n\nFeature Vectors\n\n\xe2\x80\xa2 Input data are represented by a vector of features, X\n\xe2\x80\xa2 Each vector X is a list of (attribute, value) pairs.\n\xe2\x80\xa2 Ex: X =[nose:big, teeth:big, eyes:big, moustache:no]\n\xe2\x80\xa2 The number of attributes is fixed (positive, finite)\n\xe2\x80\xa2 Each attribute has a fixed, finite number of possible values\n\xe2\x80\xa2 Each example can be interpreted as a point in a n-dimensional feature space,\n\nwhere n is the number of attributes (features)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.12\n\nSome Machine Learning Techniques\n\nProbabilistic Methods\n\n\xe2\x80\xa2 e.g., Na\xc3\xafve Bayes Classifier\n\nDecision Trees\n\n\xe2\x80\xa2 Use only discriminating features as questions in a big if-then-else tree\n\nNeural Networks\n\n\xe2\x80\xa2 Also called parallel distributed processing or connectionist systems\n\xe2\x80\xa2 Intelligence arise from having a large number of simple computational units\n\nNB: Deep Learning \xe2\x89\x88 Neural Networks \xe2\x80\x9con steroids\xe2\x80\x9d\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.13\n\nSupervised Learning\n\nLabeled Data\nIn Supervised Learning, we train a system using data with known labels.\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.14\n\nUnsupervised Learning\n\nUnlabeled Data\nIn Unsupervised Learning, we have only unlabeled data and train a system without\nguidance from an expected output.\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.15\n\nReinforcement Learning\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.16\n\nAI Learns to Park\n\nhttps://www.youtube.com/watch?v=VMp6pq6_QjI\n\nhttps://www.youtube.com/watch?v=VMp6pq6_QjI\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.17\n\nMachine Learning Categories\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.18\n\nhttp://www.cognub.com/index.php/cognitive-platform/\n\nhttp://www.cognub.com/index.php/cognitive-platform/\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.19\n\nGeneral machine learning process\n\n\xe2\x86\x92 Worksheet #6: Task 1\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.20\n\nOutline\n\n1 Machine Learning Primer\n\n2 Clustering Documents\nMotivation\nk-Means Clustering\nApplication Example\n\n3 Classifications & Predictions\n\n4 Machine Learning Evaluation\n\n5 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.21\n\nMath with Words\n\nVector Space Model\n\n\xe2\x80\xa2 A mathematical model to portray an n-dimensional space\n\n\xe2\x80\xa2 Entities are described by vectors with n coordinates in a real space Rn\n\n\xe2\x80\xa2 Given two vectors, we can compute a similarity coefficient between them\n\n\xe2\x80\xa2 Cosine of the angle between two vectors reflects their degree of similarity\n\ntf = 1 + log(tft,d ) (1)\n\nidf = log\nN\ndft\n\n(2)\n\ncos(~q , ~d ) =\n\xe2\x88\x91|v|\n\ni=1 qi \xc2\xb7 di\xe2\x88\x9a\xe2\x88\x91|v|\ni=1 q i\n\n2 \xc2\xb7\n\xe2\x88\x9a\xe2\x88\x91|v|\n\ni=1 d i\n2\n\n(3)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.22\n\nMotivation\n\nIntelligent Systems for Investigative Journalism\nOrganize large, unstructured document collections:\n\n\xe2\x80\xa2 Enron email dataset \xe2\x80\x93 ca. 500,000 emails from management\n\xe2\x80\xa2 Wikileaks \xe2\x80\x93 often releases millions of documents\n\n\xe2\x80\xa2 Guantanamo Bay Files, TPP Agreements, CIA Documents, German BND-NSA\nInquiry, . . .\n\n\xe2\x80\xa2 Facebook internal documents leaks (Cambridge Analytica scandal, 7000\ndocuments)\n\n\xe2\x80\xa2 Luanda Leaks (715,000 emails, charts, contracts, audits, etc.)\n\xe2\x80\xa2 Paradise Papers (13.4 million confidential papers regarding offshore\n\ninvestments)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.23\n\nhttps://www.thestar.com/news/paradise-papers/2019/01/29/\n\ncanada-revenue-agency-launches-100-audits-after-paradise-papers-leak.html\n\nhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.html\nhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.html\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.24https://opensemanticsearch.org\n\nhttps://opensemanticsearch.org\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.25\n\nClustering\n\nUnsupervised Learning\n\n\xe2\x80\xa2 Remember, we do not \xe2\x80\x9cclassify\xe2\x80\x9d documents (like in \xe2\x80\x9cspam vs. ham\xe2\x80\x9d)\n\xe2\x80\xa2 Rather, we group similar documents together\n\xe2\x80\xa2 Often used as a first exploratory step in data analysis\n\n\xe2\x80\xa2 Data points (here: documents) in individual clusters can be further analyzed,\npossibly with different methods\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.26\n\nWhat are Clusters?\n\nClustering\n\n\xe2\x80\xa2 The organization of unlabeled data into similarity groups, called clusters\n\xe2\x80\xa2 A cluster is a collection of data items which are \xe2\x80\x9csimilar\xe2\x80\x9d between them, and\n\n\xe2\x80\x9cdissimilar\xe2\x80\x9d to data items in other clusters.\n\xe2\x80\xa2 Generally, there is no right or wrong answer to what the clusters in a dataset\n\nare.\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.27\n\nClustering Techniques\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.28\n\nk-Means Clustering\n\nPartition-based Clustering\nK-means (MacQueen, 1967) is a partitional clustering algorithm:\n\n\xe2\x80\xa2 Given m vectors in an n-dimensional space, ~x1, . . . , ~xm \xe2\x88\x88 Rn\n\n\xe2\x80\xa2 User defines k , the number of clusters\n\nAlgorithm\n\n1 Pick k points from the dataset (usually at random).\nThese points represent our initial group centro\xc3\xafds.\n\n2 Assign each data point ~xi to the nearest centro\xc3\xafd.\n3 When all data points have been assigned, recalculate the positions of the k\n\ncentro\xc3\xafds as the average of the cluster.\n4 Repeat Steps 2 and 3 until none of the data instances change group\n\n(or changes stay below a given convergence limit \xe2\x88\x86).\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.29\n\nEuclidian Distance\n\nTo find the nearest centro\xc3\xafd:\n\xe2\x80\xa2 a possible metric is the\n\nEuclidean distance\n\xe2\x80\xa2 distance d between 2 points p, q\n\np = (p1,p2, . . . ,pn)\nq = (q1,q2, . . . ,qn)\n\nd =\n\n\xe2\x88\x9a\xe2\x88\x9a\xe2\x88\x9a\xe2\x88\x9a n\xe2\x88\x91\ni=1\n\n(pi \xe2\x88\x92 qi )2\n\n\xe2\x80\xa2 where to assign a data point ~x?\n\xe2\x80\xa2 \xe2\x86\x92 for all k clusters, choose the one\n\nwhere ~x has the smallest distance\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.30\n\nExample (1/5)\n2D-vectors, k=3: Initialize random centro\xc3\xafds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.31\n\nExample (2/5)\nPartition data points to closest centro\xc3\xafds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.32\n\nExample (3/5)\nCompute new centro\xc3\xafds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.33\n\nExample (4/5)\nRe-assign data points to closest new centro\xc3\xafds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.34\n\nExample (5/5)\nRepeat until clusters stabilize\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.35\n\nk-Means Clustering Illustrated\n\nhttps://www.youtube.com/watch?v=5I3Ei69I40s\n\xe2\x86\x92 Worksheet #6: Task 2\n\nhttps://www.youtube.com/watch?v=5I3Ei69I40s\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.36\n\nk-Means: Pros & Cons\n\nPros\n\n\xe2\x80\xa2 Simple, easy to understand and implement\n\xe2\x80\xa2 Converges very fast\n\xe2\x80\xa2 Efficient: Time complexity O(t \xc2\xb7 k \xc2\xb7 n), with\n\n\xe2\x80\xa2 n number of data points\n\xe2\x80\xa2 k number of clusters\n\xe2\x80\xa2 t number of iterations\n\n\xe2\x86\x92 considered linear for practical purposes\n\nCons\n\n\xe2\x80\xa2 User needs to choose k (usually not known)\n\xe2\x80\xa2 Sensitive to outliers\n\xe2\x80\xa2 Different results on same dataset, based on initial (random) centro\xc3\xafds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.37\n\nk-Means & Outliers\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.38\n\nk-Means: Sensitivity to Initial Seeds\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.39\n\nk-Means\n\nSummary\n\n\xe2\x80\xa2 Despite weaknesses, k-means is still one of the most popular algorithms, due\nto its simplicity and efficiency\n\n\xe2\x80\xa2 No clear evidence that any other clustering algorithm performs better in general\n\xe2\x80\xa2 Comparing different clustering algorithms is a difficult task:\n\nNo one knows the correct clusters!\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.40\n\nDocument Clustering Example: Analyzing NSF Research Grants\n\nhttps://www.youtube.com/watch?v=85fZcK5EpnA\n\nhttps://www.youtube.com/watch?v=85fZcK5EpnA\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.41\n\nOutline\n\n1 Machine Learning Primer\n\n2 Clustering Documents\n\n3 Classifications & Predictions\nIntroduction\nClassification with kNN\nRegression with kNN\n\n4 Machine Learning Evaluation\n\n5 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.42\n\nClassification of Data\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.43\n\nClassification Algorithms\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.44\n\nk-Nearest-Neighbor (kNN) Classification\n\nkNN Algorithm\n\nTraining: only store feature vectors + class labels\nTesting: Find the k data points nearest (e.g., Euclidian distance) to the new\n\nvalue. Resulting class is decided by majority vote.\n\nNote: in this simple form, kNN has no training effort, but large testing effort\n(so-called lazy learning)\n\nCopyright Antti Ajanki (https://commons.wikimedia.org/wiki/File:KnnClassification.svg), \xe2\x80\x9cKnnClassification\xe2\x80\x9d,\nlicensed under https://creativecommons.org/licenses/by- sa/3.0/legalcode\n\nhttps://commons.wikimedia.org/wiki/File:KnnClassification.svg\nhttps://creativecommons.org/licenses/by-sa/3.0/legalcode\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.45\n\nkNN Classification\n\nWith k = 1\n\n\xe2\x80\xa2 Compute the distance of the unknown sample to all existing samples\n\xe2\x80\xa2 Assign the class of the closest neighbor to the new sample\n\n\xe2\x80\xa2 Distance can be computed with different metrics, e.g.,\nEuclidean distance or Manhattan distance\n\nCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.46\n\nkNN Classification: General case\n\nWith arbitrary k\n\n\xe2\x80\xa2 kNN classification becomes a voting algorithm\n\xe2\x80\xa2 assign the same class as the majority of the k closest neighbors to the new\n\nsample\n\xe2\x80\xa2 Choice of k is dependent on data set\n\nCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.47\n\nNetflix: Predict Success of Original Content\n\nIn 2013, Netflix decided to commission two seasons of the U.S. remake of the\nBritish series House of Cards based on an analysis of its customers\xe2\x80\x99 data\n\nhttps://informationstrategyrsm.wordpress.com/2014/10/19/\n\nbig-data-analytics-house-of-cards-and-future-of-television-creation-consumption/\n\n\xe2\x86\x92 Worksheet #6: Task 3\n\nhttps://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/\nhttps://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.48\n\nRegression\nForecasting or predicting a value: e.g., house price, movie rating, temperature at noon, ...\n\nhttp://www.cognub.com/index.php/cognitive-platform/\n\nhttp://www.cognub.com/index.php/cognitive-platform/\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.49\n\nkNN Regression\n\nWith k = 1\n\n\xe2\x80\xa2 Find the nearest existing data point to a new sample as before\n\xe2\x80\xa2 Assign the value of this point (e.g., price, rating, ...) to the new instance\n\n\xe2\x80\xa2 Note: given n-dimensional vectors, we are using n \xe2\x88\x92 1 dimensions for the similarity\nand the final for the predicted value\n\nCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.50\n\nkNN Regression: General Case\n\nFind the k nearest existing data points\nAssign the average of their values to the new point\n\n\xe2\x80\xa2 Note that this algorithm cannot extrapolate\n\nCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]\n\n\xe2\x86\x92 Worksheet #6: Task 4\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.51\n\nMachine Learning at Netflix\n\nhttps://www.youtube.com/watch?v=X9ZES-fsxgU\n\nhttps://www.youtube.com/watch?v=X9ZES-fsxgU\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.52\n\nOutline\n\n1 Machine Learning Primer\n\n2 Clustering Documents\n\n3 Classifications & Predictions\n\n4 Machine Learning Evaluation\nEvaluation Methodology\nEvaluation Metrics\nError Analysis\nOverfitting\nUnderfitting\nCross-Validation\n\n5 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.53\n\nEvaluation of a ML Model\n\nMethodology\n\n\xe2\x80\xa2 How do you know if what you learned is correct?\n\xe2\x80\xa2 You run your classifier on a data set of unseen examples (that you did not use\n\nfor training) for which you know the correct classification (\xe2\x80\x9cgold standard\xe2\x80\x9d)\n\nTraining vs. testing data\n\n\xe2\x80\xa2 Split data into training (80%) and testing (20%) sets\n\xe2\x80\xa2 Depending on ML algorithm, the training set can be further split into:\n\n\xe2\x80\xa2 Actual training set (80%)\n\xe2\x80\xa2 Validation set (20%)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.54\n\nStandard Methodology\n\n1 Collect a large set of examples (all with correct classifications)\n2 Divide collection into training, validation and test set\n3 Apply learning algorithm to training set to learn the parameters\n4 Measure performance with the validation set, and adjust hyper-parameters to\n\nimprove performance\n5 Performance not good enough? \xe2\x87\x92 3\n6 Measure performance with the test set\n\nDO NOT LOOK AT THE TEST SET\nuntil you arrived at Step 6.\n\nParameters\nBasic values learned by the ML\nmodel, e.g.:\n\n\xe2\x80\xa2 for NB: prior & conditional\nprobabilities\n\n\xe2\x80\xa2 for DTs: features to split\n\xe2\x80\xa2 for ANNs: weights\n\nHyper-Parameters\nParameters used to set up the ML\nmodel, e.g.:\n\n\xe2\x80\xa2 for NB: value of delta for\nsmoothing\n\n\xe2\x80\xa2 for DTs: pruning level\n\xe2\x80\xa2 for ANNs: # of hidden layers, # of\n\nnodes per layer. . .\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.55\n\nMetrics\n\nAccuracy\n\n\xe2\x80\xa2 % of instances of the test set the algorithm correctly classifies\n\xe2\x80\xa2 when all classes are equally important and represented\n\nRecall & Precision\n\n\xe2\x80\xa2 when one class is more important than the others\n\nF-Measure\n\n\xe2\x80\xa2 Combined Precision & Recall (harmonic mean)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.56\n\nML Evaluation\n\nEvaluation of Classifiers\nWhat kind of errors can we make?\n\nReality says. . .\nPositive Negative\n\nModel predicts. . .\nPositive True Positive (TP) False Positive (FP)\nNegative False Negative (FN) True Negative (TN)\n\nThis is a so-called (binary) confusion matrix\n\nError Types\n\n\xe2\x80\xa2 False positive classification: Type I error\n(\xe2\x80\x9cconvict the innocent!\xe2\x80\x9d)\n\n\xe2\x80\xa2 False negative classification: Type II error\n(\xe2\x80\x9cfree the guilty!\xe2\x80\x9d)\n\nImportant realization: not all errors are created equal!\n\nVoltaire: \xe2\x80\x9cIt is better to risk saving a guilty man than to condemn an innocent one.\xe2\x80\x9d\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.57\n\nEvaluation Metrics\n\nCommonly used\n\n\xe2\x80\xa2 Accuracy = (TP + TN)/(P + N)\n\xe2\x80\xa2 Recall = TP/(TP + FN)\n\xe2\x80\xa2 Precision = TP/(TP + FP)\n\xe2\x80\xa2 F1-score = 2\xc2\xb7Precision\xc2\xb7RecallPrecision+Recall (harmonic mean)\n\nMind the evaluation task\nPrecision, recall etc. are defined slightly differently\nfor:\n\n\xe2\x80\xa2 Information retrieval tasks\n\xe2\x80\xa2 Classification tasks\n\xe2\x80\xa2 Ranked retrieval tasks\n\xe2\x80\xa2 Information extraction tasks\n\n\xe2\x86\x92 Worksheet #6: Task 5\n\nCopyright by Walber (https://commons.wikimedia.org/wiki/File:Precisionrecall.svg), licensed under the Creative\nCommons Attribution-Share Alike 4.0 International license\nhttps://creativecommons.org/licenses/by- sa/4.0/legalcode\n\n\xe2\x86\x92 Worksheet #6: Task 5\n\nhttps://commons.wikimedia.org/wiki/File:Precisionrecall.svg\nhttps://creativecommons.org/licenses/by-sa/4.0/legalcode\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.58\n\nConfusion Matrix\n\n\xe2\x80\xa2 Where did the learner go wrong ?\n\xe2\x80\xa2 Use a confusion matrix (contingency table)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.59\n\nLearning Curve\n\nCopyright 2007\xe2\x80\x932019, scikit-learn developers (BSD License), https://scikit- learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n\nPlot evaluation metric vs. size of training set\n\n\xe2\x80\xa2 the more, the better\n\xe2\x80\xa2 but after a while, not much improvement. . .\n\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.60\n\nSome Words on Training. . .\n\nWatch out for:\n\n\xe2\x80\xa2 Noisy Data\n\xe2\x80\xa2 Overfitting/Underfitting\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.61\n\nNoisy Data\n\nCommon issues\n\n\xe2\x80\xa2 Two examples have the same feature-value pairs, but different outputs\n\xe2\x80\xa2 Some values of features are incorrect or missing (ex. errors in the data\n\nacquisition)\n\xe2\x80\xa2 Some relevant attributes are not taken into account in the data set\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.62\n\nOverfitting\n\n\xe2\x80\xa2 If a large number of irrelevant\nfeatures are there, we may find\nmeaningless regularities in the data\nthat are particular to the training data\nbut irrelevant to the problem.\n\n\xe2\x80\xa2 Complicated boundaries overfit the\ndata (a.k.a. overtraining)\n\n\xe2\x80\xa2 they are too tuned to the particular\ntraining data at hand\n\n\xe2\x80\xa2 They do not generalize well to the\nnew data\n\n\xe2\x80\xa2 Extreme case: \xe2\x80\x9crote learning\xe2\x80\x9d\n\xe2\x80\xa2 Training error is low\n\xe2\x80\xa2 Testing error is high\n\nCopyright by Chabacano (https://commons.wikimedia.org/wiki/File:Overfitting.svg) license under the Creative Commons Attribution-Share\nAlike 4.0 International license, https://creativecommons.org/licenses/by- sa/4.0/legalcode\n\nhttps://commons.wikimedia.org/wiki/File:Overfitting.svg\nhttps://creativecommons.org/licenses/by-sa/4.0/legalcode\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.63\n\nUnderfitting\n\n\xe2\x80\xa2 We can also underfit data, i.e. use\ntoo simple decision boundary\n\n\xe2\x80\xa2 Model is not expressive enough (not\nenough features)\n\n\xe2\x80\xa2 a.k.a. Undertraining\n\xe2\x80\xa2 There is no way to fit a linear\n\ndecision boundary so that the\ntraining examples are well separated\n\n\xe2\x80\xa2 Training error is high\n\xe2\x80\xa2 Testing error is high\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.64\n\nExample: Animal Classification\n\nFeatures\nWhat about cat vs. dog?\n\n[from: Alison Cawsey: The Essence of AI (1997)]\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.65\n\nCross-Validation\n\nData Scarcity\n\n\xe2\x80\xa2 there is never enough training data\n\xe2\x80\xa2 so testing data is precious as well\n\nk-fold Cross-Validation\n\xe2\x80\x98Re-use\xe2\x80\x99 different parts of the training data for testing. E.g., 10-fold cross-validation:\n\n\xe2\x80\xa2 split data into 10 equal parts\n\xe2\x80\xa2 train on 9 of these, test on the 10th\n\xe2\x80\xa2 repeat 10 times, resulting in 10 different performance results\n\xe2\x80\xa2 average these for overall performance\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.66\n\nOutline\n\n1 Machine Learning Primer\n\n2 Clustering Documents\n\n3 Classifications & Predictions\n\n4 Machine Learning Evaluation\n\n5 Notes and Further Reading\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.67\n\nReading Material\n\nRequired\n\n\xe2\x80\xa2 [MG17, Chapters 2, 3, 5] (kNN, k-Means, Evaluation)\n\nSupplemental\n\n\xe2\x80\xa2 [PS12, Chapter 7] (ML Training)\n\xe2\x80\xa2 [PS12, Chapter 8] (Testing and Evaluation)\n\n\n\nRen\xc3\xa9 Witte\n\nMachine Learning\nPrimer\nHistory\n\nML Types\n\nProcess\n\nClustering Documents\nMotivation\n\nk-Means Clustering\n\nApplication Example\n\nClassifications &\nPredictions\nIntroduction\n\nClassification with kNN\n\nRegression with kNN\n\nMachine Learning\nEvaluation\nEvaluation Methodology\n\nEvaluation Metrics\n\nError Analysis\n\nOverfitting\n\nUnderfitting\n\nCross-Validation\n\nNotes and Further\nReading\n\n7.68\n\nReferences\n\n[MG17] Andreas C M\xc3\xbcller and Sarah Guido.\nIntroduction to Machine Learning with Python.\nO\xe2\x80\x99Reilly, 2017.\nhttps://concordiauniversity.on.worldcat.org/oclc/960211579.\n\n[PS12] James Pustejovsky and Amber Stubbs.\nNatural Language Annotation for Machine Learning.\nO\xe2\x80\x99Reilly, 2012.\nhttps://concordiauniversity.on.worldcat.org/oclc/801812987.\n\nhttps://concordiauniversity.on.worldcat.org/oclc/960211579\nhttps://concordiauniversity.on.worldcat.org/oclc/801812987\n\n\tIntro to ML\n\tMachine Learning Primer\n\tHistory\n\tML Types\n\tProcess\n\n\tClustering Documents\n\tMotivation\n\tk-Means Clustering\n\tApplication Example\n\n\tClassifications & Predictions\n\tIntroduction\n\tClassification with kNN\n\tRegression with kNN\n\n\tMachine Learning Evaluation\n\tEvaluation Methodology\n\tEvaluation Metrics\n\tError Analysis\n\tOverfitting\n\tUnderfitting\n\tCross-Validation\n\n\tNotes and Further Reading\n\n\n'