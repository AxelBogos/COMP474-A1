b'Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.1Lecture 7Machine Learning for Intelligent SystemsIntroduction, Clustering, Classification, Regression, EvaluationCOMP 474/6741, Winter 2021Ren\xc3\xa9 WitteDepartment of Computer Scienceand Software EngineeringConcordia UniversityRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.2Outline1 Machine Learning PrimerHistoryML TypesProcess2 Clustering DocumentsMotivationk-Means ClusteringApplication Example3 Classifications & PredictionsIntroductionClassification with kNNRegression with kNN4 Machine Learning EvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-Validation5 Notes and Further ReadingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.3AI, ML, DLhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bhttps://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.4HistoryLearn from experienceIn 1959, Arthur Samuel first proposed the conceptMachine Learning:\xe2\x80\x9cA computer program is said to learn fromexperience E with respect to some class of tasksT and performance measure P if its performanceat tasks in T, as measured by P, improves withexperience E.\xe2\x80\x9dRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.5Automated ReasoningInferenceProcess of deriving new facts from a set of premisesTypes of logical inference1 Deduction2 Abduction3 InductionRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.6Deductionaka Natural Deduction\xe2\x80\xa2 Conclusion follows necessary from the premises.\xe2\x80\xa2 From A\xe2\x87\x92 B and A, we conclude that B\xe2\x80\xa2 We conclude from the general case to a specific example of the general case\xe2\x80\xa2 Example:1 All men are mortal.2 Socrates is a man.3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 Socrates is mortal.\xe2\x80\xa2 Our subclass inference in RDFS also falls into this category.Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.7AbductionAbductive Reasoning\xe2\x80\xa2 Conclusion is one hypothetical (most probable) explanation for the premises\xe2\x80\xa2 From A\xe2\x87\x92 B and B, we conclude A\xe2\x80\xa2 Example:1 Drunk people do not walk straight.2 John does not walk straight.3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 John is drunk.\xe2\x80\xa2 Not sound. . . but may be most likely explanation for B\xe2\x80\xa2 Used in medicine. . .1 in reality: disease\xe2\x87\x92 symptoms2 patient complains about some symptoms. . . doctor concludes a diseaseRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.8InductionInductive Reasoning\xe2\x80\xa2 Conclusion about all members of a class from the examination of only a fewmember of the class.\xe2\x80\xa2 From A\xe2\x88\xa7C\xe2\x87\x92 B and A\xe2\x88\xa7D\xe2\x87\x92 B, we conclude A\xe2\x87\x92B\xe2\x80\xa2 We construct a general explanation based on specific cases\xe2\x80\xa2 Example:1 All CS students in COMP 474 are smart.2 All CS students on vacation are smart.3 from 1 \xe2\x88\xa7 2 \xe2\x87\x92 All CS students are smart.\xe2\x80\xa2 Not sound\xe2\x80\xa2 But, can be seen as hypothesis construction or generalisationRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.9Inductive LearningLearning from examples\xe2\x80\xa2 Most work in ML\xe2\x80\xa2 Examples are given (positive and/or negative) to train a system in aclassification (or regression) task\xe2\x80\xa2 Extrapolate from the training set to make accurate predictions about futureexamples\xe2\x80\xa2 Given a new instance X you have never seen, you must find an estimate of thefunction f(X) where f(X) is the desired outputFrom datascience.com, https://towardsdatascience.com/cat- dog- or- elon- musk- 145658489730https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.10Example\xe2\x80\xa2 Given pairs (X , f (X )) (the training set \xe2\x80\x93 the data points)\xe2\x80\xa2 Find a function f that fits the training set well\xe2\x80\xa2 So that given a new X , you can predict its f (X ) valueNote: choosing one function over another beyond just looking at the training set iscalled inductive bias (eg. prefer \xe2\x80\x9csmoother\xe2\x80\x9d functions)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.11Inductive Learning FrameworkFeature Vectors\xe2\x80\xa2 Input data are represented by a vector of features, X\xe2\x80\xa2 Each vector X is a list of (attribute, value) pairs.\xe2\x80\xa2 Ex: X =[nose:big, teeth:big, eyes:big, moustache:no]\xe2\x80\xa2 The number of attributes is fixed (positive, finite)\xe2\x80\xa2 Each attribute has a fixed, finite number of possible values\xe2\x80\xa2 Each example can be interpreted as a point in a n-dimensional feature space,where n is the number of attributes (features)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.12Some Machine Learning TechniquesProbabilistic Methods\xe2\x80\xa2 e.g., Na\xc3\xafve Bayes ClassifierDecision Trees\xe2\x80\xa2 Use only discriminating features as questions in a big if-then-else treeNeural Networks\xe2\x80\xa2 Also called parallel distributed processing or connectionist systems\xe2\x80\xa2 Intelligence arise from having a large number of simple computational unitsNB: Deep Learning \xe2\x89\x88 Neural Networks \xe2\x80\x9con steroids\xe2\x80\x9dRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.13Supervised LearningLabeled DataIn Supervised Learning, we train a system using data with known labels.Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.14Unsupervised LearningUnlabeled DataIn Unsupervised Learning, we have only unlabeled data and train a system withoutguidance from an expected output.Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.15Reinforcement LearningRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.16AI Learns to Parkhttps://www.youtube.com/watch?v=VMp6pq6_QjIhttps://www.youtube.com/watch?v=VMp6pq6_QjIRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.17Machine Learning CategoriesRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.18http://www.cognub.com/index.php/cognitive-platform/http://www.cognub.com/index.php/cognitive-platform/Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.19General machine learning process\xe2\x86\x92 Worksheet #6: Task 1Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.20Outline1 Machine Learning Primer2 Clustering DocumentsMotivationk-Means ClusteringApplication Example3 Classifications & Predictions4 Machine Learning Evaluation5 Notes and Further ReadingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.21Math with WordsVector Space Model\xe2\x80\xa2 A mathematical model to portray an n-dimensional space\xe2\x80\xa2 Entities are described by vectors with n coordinates in a real space Rn\xe2\x80\xa2 Given two vectors, we can compute a similarity coefficient between them\xe2\x80\xa2 Cosine of the angle between two vectors reflects their degree of similaritytf = 1 + log(tft,d ) (1)idf = logNdft(2)cos(~q , ~d ) =\xe2\x88\x91|v|i=1 qi \xc2\xb7 di\xe2\x88\x9a\xe2\x88\x91|v|i=1 q i2 \xc2\xb7\xe2\x88\x9a\xe2\x88\x91|v|i=1 d i2(3)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.22MotivationIntelligent Systems for Investigative JournalismOrganize large, unstructured document collections:\xe2\x80\xa2 Enron email dataset \xe2\x80\x93 ca. 500,000 emails from management\xe2\x80\xa2 Wikileaks \xe2\x80\x93 often releases millions of documents\xe2\x80\xa2 Guantanamo Bay Files, TPP Agreements, CIA Documents, German BND-NSAInquiry, . . .\xe2\x80\xa2 Facebook internal documents leaks (Cambridge Analytica scandal, 7000documents)\xe2\x80\xa2 Luanda Leaks (715,000 emails, charts, contracts, audits, etc.)\xe2\x80\xa2 Paradise Papers (13.4 million confidential papers regarding offshoreinvestments)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.23https://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlhttps://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.htmlRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.24https://opensemanticsearch.orghttps://opensemanticsearch.orgRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.25ClusteringUnsupervised Learning\xe2\x80\xa2 Remember, we do not \xe2\x80\x9cclassify\xe2\x80\x9d documents (like in \xe2\x80\x9cspam vs. ham\xe2\x80\x9d)\xe2\x80\xa2 Rather, we group similar documents together\xe2\x80\xa2 Often used as a first exploratory step in data analysis\xe2\x80\xa2 Data points (here: documents) in individual clusters can be further analyzed,possibly with different methodsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.26What are Clusters?Clustering\xe2\x80\xa2 The organization of unlabeled data into similarity groups, called clusters\xe2\x80\xa2 A cluster is a collection of data items which are \xe2\x80\x9csimilar\xe2\x80\x9d between them, and\xe2\x80\x9cdissimilar\xe2\x80\x9d to data items in other clusters.\xe2\x80\xa2 Generally, there is no right or wrong answer to what the clusters in a datasetare.Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.27Clustering TechniquesRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.28k-Means ClusteringPartition-based ClusteringK-means (MacQueen, 1967) is a partitional clustering algorithm:\xe2\x80\xa2 Given m vectors in an n-dimensional space, ~x1, . . . , ~xm \xe2\x88\x88 Rn\xe2\x80\xa2 User defines k , the number of clustersAlgorithm1 Pick k points from the dataset (usually at random).These points represent our initial group centro\xc3\xafds.2 Assign each data point ~xi to the nearest centro\xc3\xafd.3 When all data points have been assigned, recalculate the positions of the kcentro\xc3\xafds as the average of the cluster.4 Repeat Steps 2 and 3 until none of the data instances change group(or changes stay below a given convergence limit \xe2\x88\x86).Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.29Euclidian DistanceTo find the nearest centro\xc3\xafd:\xe2\x80\xa2 a possible metric is theEuclidean distance\xe2\x80\xa2 distance d between 2 points p, qp = (p1,p2, . . . ,pn)q = (q1,q2, . . . ,qn)d =\xe2\x88\x9a\xe2\x88\x9a\xe2\x88\x9a\xe2\x88\x9a n\xe2\x88\x91i=1(pi \xe2\x88\x92 qi )2\xe2\x80\xa2 where to assign a data point ~x?\xe2\x80\xa2 \xe2\x86\x92 for all k clusters, choose the onewhere ~x has the smallest distanceRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.30Example (1/5)2D-vectors, k=3: Initialize random centro\xc3\xafdsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.31Example (2/5)Partition data points to closest centro\xc3\xafdsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.32Example (3/5)Compute new centro\xc3\xafdsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.33Example (4/5)Re-assign data points to closest new centro\xc3\xafdsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.34Example (5/5)Repeat until clusters stabilizeRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.35k-Means Clustering Illustratedhttps://www.youtube.com/watch?v=5I3Ei69I40s\xe2\x86\x92 Worksheet #6: Task 2https://www.youtube.com/watch?v=5I3Ei69I40sRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.36k-Means: Pros & ConsPros\xe2\x80\xa2 Simple, easy to understand and implement\xe2\x80\xa2 Converges very fast\xe2\x80\xa2 Efficient: Time complexity O(t \xc2\xb7 k \xc2\xb7 n), with\xe2\x80\xa2 n number of data points\xe2\x80\xa2 k number of clusters\xe2\x80\xa2 t number of iterations\xe2\x86\x92 considered linear for practical purposesCons\xe2\x80\xa2 User needs to choose k (usually not known)\xe2\x80\xa2 Sensitive to outliers\xe2\x80\xa2 Different results on same dataset, based on initial (random) centro\xc3\xafdsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.37k-Means & OutliersRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.38k-Means: Sensitivity to Initial SeedsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.39k-MeansSummary\xe2\x80\xa2 Despite weaknesses, k-means is still one of the most popular algorithms, dueto its simplicity and efficiency\xe2\x80\xa2 No clear evidence that any other clustering algorithm performs better in general\xe2\x80\xa2 Comparing different clustering algorithms is a difficult task:No one knows the correct clusters!Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.40Document Clustering Example: Analyzing NSF Research Grantshttps://www.youtube.com/watch?v=85fZcK5EpnAhttps://www.youtube.com/watch?v=85fZcK5EpnARen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.41Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & PredictionsIntroductionClassification with kNNRegression with kNN4 Machine Learning Evaluation5 Notes and Further ReadingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.42Classification of DataRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.43Classification AlgorithmsRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.44k-Nearest-Neighbor (kNN) ClassificationkNN AlgorithmTraining: only store feature vectors + class labelsTesting: Find the k data points nearest (e.g., Euclidian distance) to the newvalue. Resulting class is decided by majority vote.Note: in this simple form, kNN has no training effort, but large testing effort(so-called lazy learning)Copyright Antti Ajanki (https://commons.wikimedia.org/wiki/File:KnnClassification.svg), \xe2\x80\x9cKnnClassification\xe2\x80\x9d,licensed under https://creativecommons.org/licenses/by- sa/3.0/legalcodehttps://commons.wikimedia.org/wiki/File:KnnClassification.svghttps://creativecommons.org/licenses/by-sa/3.0/legalcodeRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.45kNN ClassificationWith k = 1\xe2\x80\xa2 Compute the distance of the unknown sample to all existing samples\xe2\x80\xa2 Assign the class of the closest neighbor to the new sample\xe2\x80\xa2 Distance can be computed with different metrics, e.g.,Euclidean distance or Manhattan distanceCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.46kNN Classification: General caseWith arbitrary k\xe2\x80\xa2 kNN classification becomes a voting algorithm\xe2\x80\xa2 assign the same class as the majority of the k closest neighbors to the newsample\xe2\x80\xa2 Choice of k is dependent on data setCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.47Netflix: Predict Success of Original ContentIn 2013, Netflix decided to commission two seasons of the U.S. remake of theBritish series House of Cards based on an analysis of its customers\xe2\x80\x99 datahttps://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/\xe2\x86\x92 Worksheet #6: Task 3https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.48RegressionForecasting or predicting a value: e.g., house price, movie rating, temperature at noon, ...http://www.cognub.com/index.php/cognitive-platform/http://www.cognub.com/index.php/cognitive-platform/Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.49kNN RegressionWith k = 1\xe2\x80\xa2 Find the nearest existing data point to a new sample as before\xe2\x80\xa2 Assign the value of this point (e.g., price, rating, ...) to the new instance\xe2\x80\xa2 Note: given n-dimensional vectors, we are using n \xe2\x88\x92 1 dimensions for the similarityand the final for the predicted valueCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.50kNN Regression: General CaseFind the k nearest existing data pointsAssign the average of their values to the new point\xe2\x80\xa2 Note that this algorithm cannot extrapolateCopyright 2017 by O\xe2\x80\x99Reilly Media, Inc., [MG17]\xe2\x86\x92 Worksheet #6: Task 4Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.51Machine Learning at Netflixhttps://www.youtube.com/watch?v=X9ZES-fsxgUhttps://www.youtube.com/watch?v=X9ZES-fsxgURen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.52Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & Predictions4 Machine Learning EvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-Validation5 Notes and Further ReadingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.53Evaluation of a ML ModelMethodology\xe2\x80\xa2 How do you know if what you learned is correct?\xe2\x80\xa2 You run your classifier on a data set of unseen examples (that you did not usefor training) for which you know the correct classification (\xe2\x80\x9cgold standard\xe2\x80\x9d)Training vs. testing data\xe2\x80\xa2 Split data into training (80%) and testing (20%) sets\xe2\x80\xa2 Depending on ML algorithm, the training set can be further split into:\xe2\x80\xa2 Actual training set (80%)\xe2\x80\xa2 Validation set (20%)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.54Standard Methodology1 Collect a large set of examples (all with correct classifications)2 Divide collection into training, validation and test set3 Apply learning algorithm to training set to learn the parameters4 Measure performance with the validation set, and adjust hyper-parameters toimprove performance5 Performance not good enough? \xe2\x87\x92 36 Measure performance with the test setDO NOT LOOK AT THE TEST SETuntil you arrived at Step 6.ParametersBasic values learned by the MLmodel, e.g.:\xe2\x80\xa2 for NB: prior & conditionalprobabilities\xe2\x80\xa2 for DTs: features to split\xe2\x80\xa2 for ANNs: weightsHyper-ParametersParameters used to set up the MLmodel, e.g.:\xe2\x80\xa2 for NB: value of delta forsmoothing\xe2\x80\xa2 for DTs: pruning level\xe2\x80\xa2 for ANNs: # of hidden layers, # ofnodes per layer. . .Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.55MetricsAccuracy\xe2\x80\xa2 % of instances of the test set the algorithm correctly classifies\xe2\x80\xa2 when all classes are equally important and representedRecall & Precision\xe2\x80\xa2 when one class is more important than the othersF-Measure\xe2\x80\xa2 Combined Precision & Recall (harmonic mean)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.56ML EvaluationEvaluation of ClassifiersWhat kind of errors can we make?Reality says. . .Positive NegativeModel predicts. . .Positive True Positive (TP) False Positive (FP)Negative False Negative (FN) True Negative (TN)This is a so-called (binary) confusion matrixError Types\xe2\x80\xa2 False positive classification: Type I error(\xe2\x80\x9cconvict the innocent!\xe2\x80\x9d)\xe2\x80\xa2 False negative classification: Type II error(\xe2\x80\x9cfree the guilty!\xe2\x80\x9d)Important realization: not all errors are created equal!Voltaire: \xe2\x80\x9cIt is better to risk saving a guilty man than to condemn an innocent one.\xe2\x80\x9dRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.57Evaluation MetricsCommonly used\xe2\x80\xa2 Accuracy = (TP + TN)/(P + N)\xe2\x80\xa2 Recall = TP/(TP + FN)\xe2\x80\xa2 Precision = TP/(TP + FP)\xe2\x80\xa2 F1-score = 2\xc2\xb7Precision\xc2\xb7RecallPrecision+Recall (harmonic mean)Mind the evaluation taskPrecision, recall etc. are defined slightly differentlyfor:\xe2\x80\xa2 Information retrieval tasks\xe2\x80\xa2 Classification tasks\xe2\x80\xa2 Ranked retrieval tasks\xe2\x80\xa2 Information extraction tasks\xe2\x86\x92 Worksheet #6: Task 5Copyright by Walber (https://commons.wikimedia.org/wiki/File:Precisionrecall.svg), licensed under the CreativeCommons Attribution-Share Alike 4.0 International licensehttps://creativecommons.org/licenses/by- sa/4.0/legalcode\xe2\x86\x92 Worksheet #6: Task 5https://commons.wikimedia.org/wiki/File:Precisionrecall.svghttps://creativecommons.org/licenses/by-sa/4.0/legalcodeRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.58Confusion Matrix\xe2\x80\xa2 Where did the learner go wrong ?\xe2\x80\xa2 Use a confusion matrix (contingency table)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.59Learning CurveCopyright 2007\xe2\x80\x932019, scikit-learn developers (BSD License), https://scikit- learn.org/stable/auto_examples/model_selection/plot_learning_curve.htmlPlot evaluation metric vs. size of training set\xe2\x80\xa2 the more, the better\xe2\x80\xa2 but after a while, not much improvement. . .https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.htmlRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.60Some Words on Training. . .Watch out for:\xe2\x80\xa2 Noisy Data\xe2\x80\xa2 Overfitting/UnderfittingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.61Noisy DataCommon issues\xe2\x80\xa2 Two examples have the same feature-value pairs, but different outputs\xe2\x80\xa2 Some values of features are incorrect or missing (ex. errors in the dataacquisition)\xe2\x80\xa2 Some relevant attributes are not taken into account in the data setRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.62Overfitting\xe2\x80\xa2 If a large number of irrelevantfeatures are there, we may findmeaningless regularities in the datathat are particular to the training databut irrelevant to the problem.\xe2\x80\xa2 Complicated boundaries overfit thedata (a.k.a. overtraining)\xe2\x80\xa2 they are too tuned to the particulartraining data at hand\xe2\x80\xa2 They do not generalize well to thenew data\xe2\x80\xa2 Extreme case: \xe2\x80\x9crote learning\xe2\x80\x9d\xe2\x80\xa2 Training error is low\xe2\x80\xa2 Testing error is highCopyright by Chabacano (https://commons.wikimedia.org/wiki/File:Overfitting.svg) license under the Creative Commons Attribution-ShareAlike 4.0 International license, https://creativecommons.org/licenses/by- sa/4.0/legalcodehttps://commons.wikimedia.org/wiki/File:Overfitting.svghttps://creativecommons.org/licenses/by-sa/4.0/legalcodeRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.63Underfitting\xe2\x80\xa2 We can also underfit data, i.e. usetoo simple decision boundary\xe2\x80\xa2 Model is not expressive enough (notenough features)\xe2\x80\xa2 a.k.a. Undertraining\xe2\x80\xa2 There is no way to fit a lineardecision boundary so that thetraining examples are well separated\xe2\x80\xa2 Training error is high\xe2\x80\xa2 Testing error is highRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.64Example: Animal ClassificationFeaturesWhat about cat vs. dog?[from: Alison Cawsey: The Essence of AI (1997)]Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.65Cross-ValidationData Scarcity\xe2\x80\xa2 there is never enough training data\xe2\x80\xa2 so testing data is precious as wellk-fold Cross-Validation\xe2\x80\x98Re-use\xe2\x80\x99 different parts of the training data for testing. E.g., 10-fold cross-validation:\xe2\x80\xa2 split data into 10 equal parts\xe2\x80\xa2 train on 9 of these, test on the 10th\xe2\x80\xa2 repeat 10 times, resulting in 10 different performance results\xe2\x80\xa2 average these for overall performanceRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.66Outline1 Machine Learning Primer2 Clustering Documents3 Classifications & Predictions4 Machine Learning Evaluation5 Notes and Further ReadingRen\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.67Reading MaterialRequired\xe2\x80\xa2 [MG17, Chapters 2, 3, 5] (kNN, k-Means, Evaluation)Supplemental\xe2\x80\xa2 [PS12, Chapter 7] (ML Training)\xe2\x80\xa2 [PS12, Chapter 8] (Testing and Evaluation)Ren\xc3\xa9 WitteMachine LearningPrimerHistoryML TypesProcessClustering DocumentsMotivationk-Means ClusteringApplication ExampleClassifications &PredictionsIntroductionClassification with kNNRegression with kNNMachine LearningEvaluationEvaluation MethodologyEvaluation MetricsError AnalysisOverfittingUnderfittingCross-ValidationNotes and FurtherReading7.68References[MG17] Andreas C M\xc3\xbcller and Sarah Guido.Introduction to Machine Learning with Python.O\xe2\x80\x99Reilly, 2017.https://concordiauniversity.on.worldcat.org/oclc/960211579.[PS12] James Pustejovsky and Amber Stubbs.Natural Language Annotation for Machine Learning.O\xe2\x80\x99Reilly, 2012.https://concordiauniversity.on.worldcat.org/oclc/801812987.https://concordiauniversity.on.worldcat.org/oclc/960211579https://concordiauniversity.on.worldcat.org/oclc/801812987\tIntro to ML\tMachine Learning Primer\tHistory\tML Types\tProcess\tClustering Documents\tMotivation\tk-Means Clustering\tApplication Example\tClassifications & Predictions\tIntroduction\tClassification with kNN\tRegression with kNN\tMachine Learning Evaluation\tEvaluation Methodology\tEvaluation Metrics\tError Analysis\tOverfitting\tUnderfitting\tCross-Validation\tNotes and Further Reading'